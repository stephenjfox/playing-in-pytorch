{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derived from reading through [\"What is `torch.nn` _really_?\"](https://pytorch.org/tutorials/beginner/nn_tutorial.html).\n",
    "* I find this generally ironic, because Jeremy likes to teach from a high-level\n",
    "* This tutorial is going to be everything from the bottom-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"http://deeplearning.net/data/mnist/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filestructure(filename):\n",
    "    if not (PATH / filename).exists():\n",
    "        content = requests.get(URL + filename).content\n",
    "        (PATH / filename).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_validation_sets(filename):\n",
    "    import pickle\n",
    "    import gzip\n",
    "    \n",
    "    with gzip.open((PATH / filename).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "        return ((x_train, y_train), (x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's actually bring in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "make_filestructure(FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a sidenote - this is __totally__ how Jeremy Howard codes:\n",
    "1. Very brief, but super-effective, blocks of code\n",
    "2. No comments, but the code is so _direct_ that the given assumption that you can code in Python is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_train, xy_valid = load_train_validation_sets(FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one_digit(images_flat):\n",
    "    from matplotlib import pyplot\n",
    "    import numpy as np\n",
    "    \n",
    "    random_index = np.random.randint(len(images_flat), size=1)[0]\n",
    "    # show our random image in grayscale\n",
    "    pyplot.imshow(images_flat[random_index].reshape((28, 28)), cmap=\"gray\")\n",
    "    print(\"images.shape =\", images_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape = (50000, 784)\n"
     ]
    }
   ],
   "source": [
    "# pass in the training set, but it doesn't really matter which one we look at\n",
    "show_one_digit(xy_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch-ify me cap'n!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def convert_np_to_torch(images_as_numpy):\n",
    "    '''\n",
    "    Args:\n",
    "        images_as_numpy: all data as np.array's, concatenated together.\n",
    "            I.e. (x_train, y_train, x_val, y_val[, x_test, y_test])\n",
    "    '''\n",
    "    return map(torch.tensor, (*images_as_numpy,))\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = convert_np_to_torch((*xy_train, *xy_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4,  ..., 8, 4, 8]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 784 from torch.Size([50000, 784])\n"
     ]
    }
   ],
   "source": [
    "n, c = x_train.shape # number of samples, columns (?)\n",
    "print(n, c, 'from', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_weights_and_biases():\n",
    "    '''Return (weights, bias) constructed from a Normal Distribution, and \"Xavier-initialized\"'''\n",
    "    import math\n",
    "    \n",
    "    weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "    weights.requires_grad_() # set requires_grad = True post-hoc\n",
    "    \n",
    "    bias = torch.zeros(10, requires_grad=True)\n",
    "    \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because PyTorch can construct optimized GPU code (with a cuDNN optimizing compiler?) we're going to write two\n",
    "  functions for use\n",
    "1. A \"softmax\" function, which predicts a probability distribution\n",
    "2. A \"model\" function, because anything that is invocable can be a PyTorch model\n",
    "  * And the gradients will still be calculated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_linear_model():\n",
    "    '''To encapsulate the shenanigans that are about to ensue, I will work in a function'''\n",
    "    \n",
    "    def log_softmax(x):\n",
    "        return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "    def model(mini_batch, weights, bias):\n",
    "        return log_softmax(mini_batch @ weights + bias)\n",
    "    \n",
    "    w, b = mnist_weights_and_biases()\n",
    "    \n",
    "    bs = 64 # batch size\n",
    "    \n",
    "    x_b = x_train[0:bs] # a mini-batch from our inputs\n",
    "    predictions = model(x_b, w, b)\n",
    "    \n",
    "    print('Prediction 1:', predictions[0])\n",
    "    print('predictions.shape', predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: tensor([-2.3130, -2.4403, -2.1679, -2.2292, -2.1659, -2.9368, -2.3876, -2.1868,\n",
      "        -2.2024, -2.2117], grad_fn=<SelectBackward>)\n",
      "predictions.shape torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "simple_linear_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for the sake of keeping the global namespace relatively unpolluted (until we get to the `torch.nn` stuff)\n",
    "  I'm going to simple rewrite `simple_linear_model`.\n",
    "* With the caviate of pulling up `log_softmax(x)` for its general utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _feeling_out_torch_sum():\n",
    "    # give me [0, 120) and arrange the as 4 blocks of 5x6 matrices\n",
    "    b = torch.arange(4 * 5 * 6).view(4, 5, 6)\n",
    "    \n",
    "    a = torch.arange(10 * 2).view(2, 10)\n",
    "    print(a)\n",
    "    print(a.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
      "tensor([10, 12, 14, 16, 18, 20, 22, 24, 26, 28])\n"
     ]
    }
   ],
   "source": [
    "_feeling_out_torch_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?torch.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _feeling_out_torch_unsqueeze():\n",
    "#     ?torch.unsqueeze\n",
    "    small_range = torch.arange(1., 3., .5)\n",
    "    print(small_range)\n",
    "    \n",
    "    _sum = small_range.sum()\n",
    "    print(_sum)\n",
    "    \n",
    "    print(_sum.unsqueeze(-1))\n",
    "    print(_sum.log())\n",
    "    print(_sum.log().unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.5000, 2.0000, 2.5000])\n",
      "tensor(7.)\n",
      "tensor([7.])\n",
      "tensor(1.9459)\n",
      "tensor([1.9459])\n"
     ]
    }
   ],
   "source": [
    "_feeling_out_torch_unsqueeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Log) Softmax, here, is the difference of\n",
    "1. the input, and\n",
    "2. the log of the sum along the last dimension of the exponential of the input\n",
    "\n",
    "Then it just boxes that log.\n",
    "* Why index `-1` rather than `0` for a scalar value? I don't know, but maybe we can find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.]]), tensor([[0.]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(1).unsqueeze(-1), torch.Tensor(1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No difference when there's one element, but how about with multiple?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00],\n",
       "         [-1.0842e-19],\n",
       "         [ 3.6898e+05]]), tensor([[ 0.0000e+00, -1.0842e-19,  2.9062e+05]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(3).unsqueeze(-1), torch.Tensor(3).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is. `unsqueeze(-1)` is guaranteed to box the elements.\n",
    "* If, for some reason, the last dimension in your sum was a vector, this `unsqueeze` would make sure that\n",
    "  you have individually boxes elements (in this case, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Another token Jeremy Howard-ism:\n",
    "* Great coding practices, but he doesn't explain them along the way.\n",
    "* He wants you to trust his implementation, and know enough to dig in yourself (as I have done above)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood(x, target):\n",
    "    return -x[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, target_batch):\n",
    "    preds = torch.argmax(out, dim=1) # this is weird to me\n",
    "    return (preds == target_batch).float().mean() # average accuracy of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?torch.argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets the indices of the maximum values of the rows of the linear regression...\n",
    "* Why is that how we determine out prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_linear_model():\n",
    "    '''To encapsulate the shenanigans that are about to ensue, I will work in a function'''\n",
    "\n",
    "    w, b = mnist_weights_and_biases()\n",
    "    \n",
    "    def model(mini_batch):\n",
    "        return log_softmax(mini_batch @ w + b)\n",
    "    \n",
    "    \n",
    "    bs = 64 # batch size\n",
    "    \n",
    "    x_b = x_train[0:bs] # a mini-batch from our inputs\n",
    "    predictions = model(x_b)\n",
    "    \n",
    "    print('Prediction 1:', predictions[0])\n",
    "    print('predictions.shape', predictions.shape)\n",
    "    \n",
    "    loss_func = neg_log_likelihood\n",
    "    \n",
    "    y_b = y_train[0:bs]\n",
    "    \n",
    "    print(loss_func(predictions, y_b))\n",
    "    \n",
    "    lr = 0.5\n",
    "    epochs = 2 # how many laps through the dataset\n",
    "\n",
    "    print(f\"training our model {epochs} epochs with learning rate of {lr}\")\n",
    "    for epoch in range(epochs):\n",
    "        # this 'i' is sometimes called a \"period\" when iterating through the dataset\n",
    "        for i in range((n - 1) // bs + 1): # why n-1?\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs # now we have a range [i*bs, (i+1)*bs)\n",
    "            \n",
    "            x_mini_batch = x_train[start_i: end_i]\n",
    "            y_mini_batch = y_train[start_i: end_i]\n",
    "            \n",
    "            prediction = model(x_mini_batch) # prediction matrix of matrices (\"tensor\")\n",
    "            loss = loss_func(prediction, y_mini_batch)\n",
    "            \n",
    "            if i % 100 == 0: # do some logging every 20th batch\n",
    "                print(f' Loss of {loss}, with batch accuracy of {accuracy(prediction, y_mini_batch)}')\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                w -= w.grad * lr\n",
    "                b -= b.grad * lr\n",
    "                w.grad.zero_()\n",
    "                b.grad.zero_()\n",
    "    \n",
    "    print(loss_func(model(x_b), y_b), accuracy(model(x_b), y_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: tensor([-2.6708, -2.2444, -2.5179, -2.3209, -2.1072, -2.6687, -1.8283, -2.4041,\n",
      "        -2.5895, -2.0480], grad_fn=<SelectBackward>)\n",
      "predictions.shape torch.Size([64, 10])\n",
      "tensor(2.3308, grad_fn=<NegBackward>)\n",
      "training our model 2 epochs with learning rate of 0.5\n",
      " Loss of 2.330822706222534, with batch accuracy of 0.109375\n",
      " Loss of 0.31124401092529297, with batch accuracy of 0.90625\n",
      " Loss of 0.2908550500869751, with batch accuracy of 0.90625\n",
      " Loss of 0.38969311118125916, with batch accuracy of 0.921875\n",
      " Loss of 0.23748202621936798, with batch accuracy of 0.90625\n",
      " Loss of 0.37772005796432495, with batch accuracy of 0.890625\n",
      " Loss of 0.2568615674972534, with batch accuracy of 0.890625\n",
      " Loss of 0.3808162808418274, with batch accuracy of 0.890625\n",
      " Loss of 0.28118157386779785, with batch accuracy of 0.921875\n",
      " Loss of 0.25753968954086304, with batch accuracy of 0.921875\n",
      " Loss of 0.19260373711585999, with batch accuracy of 0.90625\n",
      " Loss of 0.34308546781539917, with batch accuracy of 0.921875\n",
      " Loss of 0.21235303580760956, with batch accuracy of 0.921875\n",
      " Loss of 0.3512260317802429, with batch accuracy of 0.890625\n",
      " Loss of 0.22305332124233246, with batch accuracy of 0.921875\n",
      " Loss of 0.3636215329170227, with batch accuracy of 0.890625\n",
      "tensor(0.2269, grad_fn=<NegBackward>) tensor(0.9531)\n"
     ]
    }
   ],
   "source": [
    "simple_linear_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def functional_linear_model():\n",
    "    '''Use torch.functional'''\n",
    "    weights, bias = mnist_weights_and_biases()\n",
    "    \n",
    "    bs = 64 # batch_size\n",
    "\n",
    "    xb = x_train[0:bs]\n",
    "    yb = y_train[0:bs]\n",
    "    loss_func = F.cross_entropy\n",
    "\n",
    "    def model(xb):\n",
    "        return xb @ weights + bias\n",
    "    \n",
    "    print('Prediction #1 loss:', loss_func(model(xb), yb), 'Accuracy:', accuracy(model(xb), yb))\n",
    "    \n",
    "    lr = 0.5\n",
    "    epochs = 2 # how many laps through the dataset\n",
    "\n",
    "    print(f\"training our model {epochs} epochs with learning rate of {lr}\")\n",
    "    for epoch in range(epochs):\n",
    "        # this 'i' is sometimes called a \"period\" when iterating through the dataset\n",
    "        for i in range((n - 1) // bs + 1): # why n-1?\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs # now we have a range [i*bs, (i+1)*bs)\n",
    "            \n",
    "            x_mini_batch = x_train[start_i: end_i]\n",
    "            y_mini_batch = y_train[start_i: end_i]\n",
    "            \n",
    "            prediction = model(x_mini_batch) # prediction matrix of matrices (\"tensor\")\n",
    "            loss = loss_func(prediction, y_mini_batch)\n",
    "            \n",
    "            if i % 100 == 0: # do some logging every 20th batch\n",
    "                print(f' Loss of {loss}, with batch accuracy of {accuracy(prediction, y_mini_batch)}')\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                weights -= weights.grad * lr\n",
    "                bias -= bias.grad * lr\n",
    "                weights.grad.zero_()\n",
    "                bias.grad.zero_()\n",
    "    \n",
    "    print('Final Loss', loss_func(model(xb), yb), 'and Accuracy', accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction #1 loss: tensor(2.3569, grad_fn=<NllLossBackward>) Accuracy: tensor(0.0625)\n",
      "training our model 2 epochs with learning rate of 0.5\n",
      " Loss of 2.3569226264953613, with batch accuracy of 0.0625\n",
      " Loss of 0.3186066746711731, with batch accuracy of 0.890625\n",
      " Loss of 0.3051021993160248, with batch accuracy of 0.890625\n",
      " Loss of 0.3864426612854004, with batch accuracy of 0.921875\n",
      " Loss of 0.23489728569984436, with batch accuracy of 0.890625\n",
      " Loss of 0.3852415084838867, with batch accuracy of 0.890625\n",
      " Loss of 0.26294535398483276, with batch accuracy of 0.890625\n",
      " Loss of 0.37971171736717224, with batch accuracy of 0.890625\n",
      " Loss of 0.27945461869239807, with batch accuracy of 0.921875\n",
      " Loss of 0.2591649889945984, with batch accuracy of 0.921875\n",
      " Loss of 0.19632761180400848, with batch accuracy of 0.90625\n",
      " Loss of 0.3407357931137085, with batch accuracy of 0.921875\n",
      " Loss of 0.2096318006515503, with batch accuracy of 0.9375\n",
      " Loss of 0.35662347078323364, with batch accuracy of 0.90625\n",
      " Loss of 0.22683103382587433, with batch accuracy of 0.90625\n",
      " Loss of 0.36332547664642334, with batch accuracy of 0.890625\n",
      "Final Loss tensor(0.2271, grad_fn=<NllLossBackward>) and Accuracy tensor(0.9531)\n"
     ]
    }
   ],
   "source": [
    "functional_linear_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor into PyTorch nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    '''Encapsulates the weights, bias, and forward() method'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "lr = 0.5\n",
    "xb = x_train[0:bs] # one batch\n",
    "yb = y_train[0:bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3358, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(F.cross_entropy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(2.3358, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.0469)\n",
      "Loss: tensor(0.4071, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3210, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.2699, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3044, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8750)\n",
      "Loss: tensor(0.5514, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3868, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.3048, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2362, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.2110, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3737, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.3655, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2598, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.2252, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3784, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9062)\n",
      "Loss: tensor(0.2158, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2741, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.3352, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2617, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.2002, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.1988, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9062)\n",
      "Loss: tensor(0.4837, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3392, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9375)\n",
      "Loss: tensor(0.2568, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2096, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9375)\n",
      "Loss: tensor(0.1743, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3494, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.3261, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2259, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.1960, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3604, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.1906, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def fit(epochs=2):\n",
    "    '''Train the model to fit to the data distribution'''\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        steps = (n - 1) // bs + 1\n",
    "        for i in range(steps):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb, yb = x_train[start_i:end_i], y_train[start_i:end_i]\n",
    "            \n",
    "            pred = model(xb)\n",
    "            loss = F.cross_entropy(pred, yb)\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(\"Loss:\", loss)\n",
    "            if i % 100 == 0:\n",
    "                print(\"Accuracy:\", accuracy(pred, yb))\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor for nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2945, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_global_loss():\n",
    "    print('Global Loss:', loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(2.2945, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.0938)\n",
      "Loss: tensor(0.4156, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3107, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.2771, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2985, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.5565, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3838, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.3042, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2380, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.2121, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3795, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.3640, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2651, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.2284, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3797, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9062)\n",
      "Loss: tensor(0.2145, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2764, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.3347, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2597, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.2012, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.1954, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9062)\n",
      "Loss: tensor(0.4870, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3401, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.2548, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2109, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9375)\n",
      "Loss: tensor(0.1754, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3537, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.3261, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2280, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9062)\n",
      "Loss: tensor(0.1975, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3633, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.1891, grad_fn=<NllLossBackward>)\n",
      "Global Loss: tensor(0.2251, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "fit()\n",
    "\n",
    "print_global_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor to use torch.optim\n",
    "\n",
    "For all your optimization needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Loss: tensor(2.3401, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "print_global_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Loss: tensor(0.0812, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print_global_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor to use Dataset\n",
    "\n",
    "By defining how we index into our dataset, we can iterate and slice like a serial killer.\n",
    "* I haven't yet done the Dataset tutorial, because it might not be as necessary for my work.\n",
    "  + What I have done is read the mentality of the Dataset API when I was working with Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Loss: tensor(2.3164, grad_fn=<NllLossBackward>)\n",
      "Global Loss: tensor(0.0827, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "print_global_loss()\n",
    "\n",
    "for e in range(epochs):\n",
    "    steps = (n - 1) // bs + 1\n",
    "    for i in range(steps):\n",
    "        start = i * bs\n",
    "        xb, yb = train_ds[start:start + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print_global_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tower of abstractions: DataLoader\n",
    "\n",
    "You can wrap a `Dataset` in a `DataLoader` to get a better grip on things.\n",
    "* Even easier to iterate. We don't control the indices anymore!\n",
    "\n",
    "In so doing, we:\n",
    "* Off-load batching\n",
    "* Remove concerns for \"Index Out of Bounds\" errors\n",
    "* Leverage the ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Loss: tensor(2.3418, grad_fn=<NllLossBackward>)\n",
      "Global Loss: tensor(0.0810, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "print_global_loss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print_global_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick aside\n",
    "\n",
    "Till this point, we've been doing a lot of trimming and refactoring to get a solid training loop\n",
    "* We're down to ~7 lines of code (which is a something of an ideal)\n",
    "* We __haven't__ given any consideration to a train/validation/test data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate or die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "# no shuffle, because it's just inference and the network isn't \"learning\"\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything before this is considered \"step 1\".\n",
    "* `model.train()` is a switch that sets a flag for things like `nn.BatchNorm2d` and `nn.Dropout`\n",
    "* `model.eval()` is a switch for when you're evaluating your model, employing it for _inference_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(0.2978)\n",
      "1 tensor(0.2777)\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl) # sum all the small losses\n",
    "    \n",
    "    print(epoch, valid_loss / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing our own abstraction\n",
    "\n",
    "I'm tired of copy-paste/typing the same loops out over and over.\n",
    "* It's good to know what you have to do, but it's bad to be so redundant in programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Because we've fit a given `model` to a given `_d(ata)s(et)`, we can make a training function for those\n",
    "   given batches of data\n",
    "2. We can also group our train/validation `DataLoader`s because of the shared initialization code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, f_loss, xb, yb, opt=None):\n",
    "    loss = f_loss(model(xb), yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    # this length is designed by Jeremy. I don't know it's useful. Maybe percentages?\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, f_loss, opt, train_dl, valid_dl):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        f_loss: function that computes the loss, given predictions and labels\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, f_loss, xb, yb, opt)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, f_loss, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "#             print(len(losses))\n",
    "#             print(len(nums))\n",
    "#             if epoch == 0: print((losses, nums))\n",
    "            \n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        \n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_dataset, valid_dataset, batch_size):\n",
    "    return (\n",
    "        DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(valid_dataset, batch_size=batch_size * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "79\n",
      "((0.8022516965866089, 0.7330406904220581, 0.8440415859222412, 0.9190787672996521, 0.9403488039970398, 0.6811109781265259, 0.794546902179718, 0.5706993341445923, 0.42919379472732544, 0.6954090595245361, 0.48123791813850403, 0.6090065836906433, 0.5629838109016418, 0.5737200975418091, 0.6367772221565247, 0.7905337810516357, 0.9434087872505188, 0.8313820362091064, 0.6373736262321472, 0.5125751495361328, 0.5941041111946106, 0.8983874917030334, 1.0063868761062622, 1.00661301612854, 0.8407307267189026, 0.6104289293289185, 0.27579009532928467, 0.7052438259124756, 0.5208159685134888, 0.629485011100769, 0.8378593325614929, 0.8677883148193359, 0.7785532474517822, 0.6377819776535034, 0.7012762427330017, 0.7289571166038513, 0.6604243516921997, 0.692999541759491, 0.8283705115318298, 0.5624573230743408, 0.7023231983184814, 0.7204043865203857, 0.44788095355033875, 0.666982114315033, 0.45516759157180786, 0.621056318283081, 0.48781630396842957, 0.5128905177116394, 0.6097342371940613, 0.6693629026412964, 0.705083966255188, 0.6861369609832764, 0.50139981508255, 0.5413848161697388, 0.5981243848800659, 0.6349290013313293, 0.6830460429191589, 0.7844062447547913, 0.7370913028717041, 0.8739550709724426, 0.7804494500160217, 0.5672137141227722, 0.6108085513114929, 0.5087757110595703, 0.43776869773864746, 0.5007302761077881, 0.40535223484039307, 0.5681688189506531, 0.4547482132911682, 0.33841970562934875, 0.29386380314826965, 0.27633607387542725, 0.7491857409477234, 1.0161783695220947, 0.212758406996727, 0.680790364742279, 0.5267011523246765, 0.5748451948165894, 0.4193633496761322), (128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 16))\n",
      "0 0.647268680524826\n",
      "79\n",
      "79\n",
      "1 0.29202795944213866\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch to Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)        \n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        # gotta love Jeremy's code\n",
    "        xb = xb.view(-1, 1, 28, 28) # any batch_size, 1 channel, 28x28 pixels\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        \n",
    "        # reshape the output to the second dimension of the pool size, and just fill the rest to whatever.\n",
    "        return xb.view(-1, xb.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1 # drop the learning rate a smidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to apply \"[Momentum](https://cs231n.github.io/neural-networks-3/#sgd)\", which is SGD with a memory that allows for acceleration through\n",
    "  the loss function landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_CNN()\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "79\n",
      "((0.30444860458374023, 0.3562984764575958, 0.4277384877204895, 0.5014829635620117, 0.48250144720077515, 0.3880632519721985, 0.20272275805473328, 0.32982489466667175, 0.20577040314674377, 0.3576623797416687, 0.2006949484348297, 0.26592713594436646, 0.30693739652633667, 0.296922504901886, 0.24109363555908203, 0.26987355947494507, 0.5987476110458374, 0.2588673532009125, 0.2934098243713379, 0.15217657387256622, 0.2632577419281006, 0.4800276458263397, 0.549534261226654, 0.32499027252197266, 0.24341151118278503, 0.22855380177497864, 0.17020606994628906, 0.4800698757171631, 0.30877888202667236, 0.3629472851753235, 0.49663811922073364, 0.5787115097045898, 0.2166202962398529, 0.2930310368537903, 0.35528451204299927, 0.3584819734096527, 0.21122372150421143, 0.2924754321575165, 0.423581063747406, 0.3465912938117981, 0.2591487169265747, 0.28588730096817017, 0.30690738558769226, 0.33558472990989685, 0.3036747872829437, 0.2575536072254181, 0.23342877626419067, 0.15359291434288025, 0.30962803959846497, 0.28423893451690674, 0.4044806659221649, 0.2654789686203003, 0.21568256616592407, 0.24476270377635956, 0.2180081307888031, 0.22571533918380737, 0.2880624532699585, 0.23086315393447876, 0.24399831891059875, 0.35456380248069763, 0.39111602306365967, 0.269205778837204, 0.36484581232070923, 0.20788532495498657, 0.15310366451740265, 0.2988605499267578, 0.20902866125106812, 0.298900306224823, 0.1409522444009781, 0.21371009945869446, 0.10397562384605408, 0.10979469865560532, 0.36657601594924927, 0.46541765332221985, 0.07619892805814743, 0.4491823613643646, 0.30527764558792114, 0.4317895770072937, 0.14433109760284424), (128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 16))\n",
      "0 0.30362501335144043\n",
      "79\n",
      "79\n",
      "1 0.24054722032546996\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An alternative abstraction: [`nn.Sequential`](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can only supply instances of `nn.Module` to `nn.Sequential`\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "def preprocess(x):\n",
    "    '''Reshapes an MNIST input for CNN'''\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "79\n",
      "((0.5561911463737488, 0.6966434121131897, 0.4914347231388092, 0.6605960130691528, 0.7451556921005249, 0.650911808013916, 0.5321139693260193, 0.37168675661087036, 0.2354753464460373, 0.5936785340309143, 0.4616206884384155, 0.369215726852417, 0.3910178542137146, 0.36459881067276, 0.5307605862617493, 0.4271329641342163, 0.7557274103164673, 0.42243075370788574, 0.4085313379764557, 0.23524603247642517, 0.4090055227279663, 0.6132016181945801, 0.7016457915306091, 0.486743688583374, 0.37956374883651733, 0.4555610418319702, 0.315112829208374, 0.6407705545425415, 0.4629344940185547, 0.3783039152622223, 0.39488157629966736, 0.7018197178840637, 0.29813095927238464, 0.4129551947116852, 0.4378446638584137, 0.6195954084396362, 0.3565754294395447, 0.3668956160545349, 0.6045891046524048, 0.38897472620010376, 0.34762752056121826, 0.3507446050643921, 0.4087551534175873, 0.3978545367717743, 0.5548195838928223, 0.5066040754318237, 0.30303144454956055, 0.258012056350708, 0.5292671322822571, 0.3502969443798065, 0.4994681179523468, 0.30620747804641724, 0.48659461736679077, 0.4536357522010803, 0.3583950996398926, 0.33423715829849243, 0.5322299003601074, 0.3473799228668213, 0.3891322910785675, 0.5957729816436768, 0.5426371693611145, 0.4526858925819397, 0.47495463490486145, 0.2921130061149597, 0.2993006110191345, 0.6251477003097534, 0.44791173934936523, 0.47977620363235474, 0.2780773639678955, 0.3491710424423218, 0.15870535373687744, 0.2855668365955353, 0.44846248626708984, 0.9914429187774658, 0.26934006810188293, 0.557642936706543, 0.5488032698631287, 0.311440646648407, 0.11041221022605896), (128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 16))\n",
      "0 0.4539177337169647\n",
      "79\n",
      "79\n",
      "1 0.2678316388130188\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size=4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`F.avg_pool2d` ~ `nn.AvgPool2d`\n",
    "* The last can be replaced with `nn.AdaptiveAvgPool2d` which allows you to articulate the desired _output_,\n",
    "  rather than the _input_, size; we have been doing the latter, rather than the former\n",
    "\n",
    "We've also pulled the data management into our own code, and left the tools to do what they do best.\n",
    "* Leverage the framework where useful, but code your rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4469121433734894\n",
      "1 0.31724769039154055\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def preprocess(x, y):\n",
    "    # put the data on the gpu if you can\n",
    "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.21975669264793396\n",
      "1 0.1737469123840332\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
