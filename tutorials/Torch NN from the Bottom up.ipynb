{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derived from reading through [\"What is `torch.nn` _really_?\"](https://pytorch.org/tutorials/beginner/nn_tutorial.html).\n",
    "* I find this generally ironic, because Jeremy likes to teach from a high-level\n",
    "* This tutorial is going to be everything from the bottom-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"http://deeplearning.net/data/mnist/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filestructure(filename):\n",
    "    if not (PATH / filename).exists():\n",
    "        content = requests.get(URL + filename).content\n",
    "        (PATH / filename).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_validation_sets(filename):\n",
    "    import pickle\n",
    "    import gzip\n",
    "    \n",
    "    with gzip.open((PATH / filename).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "        return ((x_train, y_train), (x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's actually bring in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "make_filestructure(FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a sidenote - this is __totally__ how Jeremy Howard codes:\n",
    "1. Very brief, but super-effective, blocks of code\n",
    "2. No comments, but the code is so _direct_ that the given assumption that you can code in Python is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_train, xy_valid = load_train_validation_sets(FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one_digit(images_flat):\n",
    "    from matplotlib import pyplot\n",
    "    import numpy as np\n",
    "    \n",
    "    random_index = np.random.randint(len(images_flat), size=1)[0]\n",
    "    # show our random image in grayscale\n",
    "    pyplot.imshow(images_flat[random_index].reshape((28, 28)), cmap=\"gray\")\n",
    "    print(\"images.shape =\", images_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape = (50000, 784)\n"
     ]
    }
   ],
   "source": [
    "# pass in the training set, but it doesn't really matter which one we look at\n",
    "show_one_digit(xy_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch-ify me cap'n!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def convert_np_to_torch(images_as_numpy):\n",
    "    '''\n",
    "    Args:\n",
    "        images_as_numpy: all data as np.array's, concatenated together.\n",
    "            I.e. (x_train, y_train, x_val, y_val[, x_test, y_test])\n",
    "    '''\n",
    "    return map(torch.tensor, (*images_as_numpy,))\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = convert_np_to_torch((*xy_train, *xy_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4,  ..., 8, 4, 8]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 784 from torch.Size([50000, 784])\n"
     ]
    }
   ],
   "source": [
    "n, c = x_train.shape # number of samples, columns (?)\n",
    "print(n, c, 'from', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_weights_and_biases():\n",
    "    '''Return (weights, bias) constructed from a Normal Distribution, and \"Xavier-initialized\"'''\n",
    "    import math\n",
    "    \n",
    "    weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "    weights.requires_grad_() # set requires_grad = True post-hoc\n",
    "    \n",
    "    bias = torch.zeros(10, requires_grad=True)\n",
    "    \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because PyTorch can construct optimized GPU code (with a cuDNN optimizing compiler?) we're going to write two\n",
    "  functions for use\n",
    "1. A \"softmax\" function, which predicts a probability distribution\n",
    "2. A \"model\" function, because anything that is invocable can be a PyTorch model\n",
    "  * And the gradients will still be calculated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_linear_model():\n",
    "    '''To encapsulate the shenanigans that are about to ensue, I will work in a function'''\n",
    "    \n",
    "    def log_softmax(x):\n",
    "        return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "    def model(mini_batch, weights, bias):\n",
    "        return log_softmax(mini_batch @ weights + bias)\n",
    "    \n",
    "    w, b = mnist_weights_and_biases()\n",
    "    \n",
    "    bs = 64 # batch size\n",
    "    \n",
    "    x_b = x_train[0:bs] # a mini-batch from our inputs\n",
    "    predictions = model(x_b, w, b)\n",
    "    \n",
    "    print('Prediction 1:', predictions[0])\n",
    "    print('predictions.shape', predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: tensor([-3.0738, -2.2553, -2.1997, -2.1826, -2.3903, -2.1997, -2.6240, -1.6742,\n",
      "        -2.5130, -2.5024], grad_fn=<SelectBackward>)\n",
      "predictions.shape torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "simple_linear_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for the sake of keeping the global namespace relatively unpolluted (until we get to the `torch.nn` stuff)\n",
    "  I'm going to simple rewrite `simple_linear_model`.\n",
    "* With the caviate of pulling up `log_softmax(x)` for its general utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _feeling_out_torch_sum():\n",
    "    # give me [0, 120) and arrange the as 4 blocks of 5x6 matrices\n",
    "    b = torch.arange(4 * 5 * 6).view(4, 5, 6)\n",
    "    \n",
    "    a = torch.arange(10 * 2).view(2, 10)\n",
    "    print(a)\n",
    "    print(a.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n",
      "tensor([10, 12, 14, 16, 18, 20, 22, 24, 26, 28])\n"
     ]
    }
   ],
   "source": [
    "_feeling_out_torch_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?torch.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _feeling_out_torch_unsqueeze():\n",
    "#     ?torch.unsqueeze\n",
    "    small_range = torch.arange(1., 3., .5)\n",
    "    print(small_range)\n",
    "    \n",
    "    _sum = small_range.sum()\n",
    "    print(_sum)\n",
    "    \n",
    "    print(_sum.unsqueeze(-1))\n",
    "    print(_sum.log())\n",
    "    print(_sum.log().unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.5000, 2.0000, 2.5000])\n",
      "tensor(7.)\n",
      "tensor([7.])\n",
      "tensor(1.9459)\n",
      "tensor([1.9459])\n"
     ]
    }
   ],
   "source": [
    "_feeling_out_torch_unsqueeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Log) Softmax, here, is the difference of\n",
    "1. the input, and\n",
    "2. the log of the sum along the last dimension of the exponential of the input\n",
    "\n",
    "Then it just boxes that log.\n",
    "* Why index `-1` rather than `0` for a scalar value? I don't know, but maybe we can find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.]]), tensor([[0.]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(1).unsqueeze(-1), torch.Tensor(1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No difference when there's one element, but how about with multiple?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00],\n",
       "         [-3.6893e+19],\n",
       "         [-5.0996e-06]]), tensor([[ 0.0000e+00, -3.6893e+19,  0.0000e+00]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(3).unsqueeze(-1), torch.Tensor(3).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is. `unsqueeze(-1)` is guaranteed to box the elements.\n",
    "* If, for some reason, the last dimension in your sum was a vector, this `unsqueeze` would make sure that\n",
    "  you have individually boxes elements (in this case, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Another token Jeremy Howard-ism:\n",
    "* Great coding practices, but he doesn't explain them along the way.\n",
    "* He wants you to trust his implementation, and know enough to dig in yourself (as I have done above)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood(x, target):\n",
    "    return -x[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, target_batch):\n",
    "    preds = torch.argmax(out, dim=1) # this is weird to me\n",
    "    return (preds == target_batch).float().mean() # average accuracy of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?torch.argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets the indices of the maximum values of the rows of the linear regression...\n",
    "* Why is that how we determine out prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_linear_model():\n",
    "    '''To encapsulate the shenanigans that are about to ensue, I will work in a function'''\n",
    "\n",
    "    w, b = mnist_weights_and_biases()\n",
    "    \n",
    "    def model(mini_batch):\n",
    "        return log_softmax(mini_batch @ w + b)\n",
    "    \n",
    "    \n",
    "    bs = 64 # batch size\n",
    "    \n",
    "    x_b = x_train[0:bs] # a mini-batch from our inputs\n",
    "    predictions = model(x_b)\n",
    "    \n",
    "    print('Prediction 1:', predictions[0])\n",
    "    print('predictions.shape', predictions.shape)\n",
    "    \n",
    "    loss_func = neg_log_likelihood\n",
    "    \n",
    "    y_b = y_train[0:bs]\n",
    "    \n",
    "    print(loss_func(predictions, y_b))\n",
    "    \n",
    "    lr = 0.5\n",
    "    epochs = 2 # how many laps through the dataset\n",
    "\n",
    "    print(f\"training our model {epochs} epochs with learning rate of {lr}\")\n",
    "    for epoch in range(epochs):\n",
    "        # this 'i' is sometimes called a \"period\" when iterating through the dataset\n",
    "        for i in range((n - 1) // bs + 1): # why n-1?\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs # now we have a range [i*bs, (i+1)*bs)\n",
    "            \n",
    "            x_mini_batch = x_train[start_i: end_i]\n",
    "            y_mini_batch = y_train[start_i: end_i]\n",
    "            \n",
    "            prediction = model(x_mini_batch) # prediction matrix of matrices (\"tensor\")\n",
    "            loss = loss_func(prediction, y_mini_batch)\n",
    "            \n",
    "            if i % 100 == 0: # do some logging every 20th batch\n",
    "                print(f' Loss of {loss}, with batch accuracy of {accuracy(prediction, y_mini_batch)}')\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                w -= w.grad * lr\n",
    "                b -= b.grad * lr\n",
    "                w.grad.zero_()\n",
    "                b.grad.zero_()\n",
    "    \n",
    "    print(loss_func(model(x_b), y_b), accuracy(model(x_b), y_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: tensor([-1.9524, -2.5062, -2.1828, -2.5593, -2.8282, -2.2362, -1.9332, -2.0642,\n",
      "        -3.0664, -2.2805], grad_fn=<SelectBackward>)\n",
      "predictions.shape torch.Size([64, 10])\n",
      "tensor(2.3146, grad_fn=<NegBackward>)\n",
      "training our model 2 epochs with learning rate of 0.5\n",
      " Loss of 2.3146328926086426, with batch accuracy of 0.0625\n",
      " Loss of 0.32288476824760437, with batch accuracy of 0.890625\n",
      " Loss of 0.2939816117286682, with batch accuracy of 0.90625\n",
      " Loss of 0.39154088497161865, with batch accuracy of 0.921875\n",
      " Loss of 0.23661889135837555, with batch accuracy of 0.90625\n",
      " Loss of 0.3833451271057129, with batch accuracy of 0.890625\n",
      " Loss of 0.2693641781806946, with batch accuracy of 0.890625\n",
      " Loss of 0.37894997000694275, with batch accuracy of 0.90625\n",
      " Loss of 0.279062420129776, with batch accuracy of 0.921875\n",
      " Loss of 0.26254570484161377, with batch accuracy of 0.921875\n",
      " Loss of 0.1923317313194275, with batch accuracy of 0.90625\n",
      " Loss of 0.3412483334541321, with batch accuracy of 0.921875\n",
      " Loss of 0.21167385578155518, with batch accuracy of 0.9375\n",
      " Loss of 0.355866402387619, with batch accuracy of 0.890625\n",
      " Loss of 0.23207753896713257, with batch accuracy of 0.90625\n",
      " Loss of 0.36180543899536133, with batch accuracy of 0.890625\n",
      "tensor(0.2270, grad_fn=<NegBackward>) tensor(0.9531)\n"
     ]
    }
   ],
   "source": [
    "simple_linear_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def functional_linear_model():\n",
    "    '''Use torch.functional'''\n",
    "    weights, bias = mnist_weights_and_biases()\n",
    "    \n",
    "    bs = 64 # batch_size\n",
    "\n",
    "    xb = x_train[0:bs]\n",
    "    yb = y_train[0:bs]\n",
    "    loss_func = F.cross_entropy\n",
    "\n",
    "    def model(xb):\n",
    "        return xb @ weights + bias\n",
    "    \n",
    "    print('Prediction #1 loss:', loss_func(model(xb), yb), 'Accuracy:', accuracy(model(xb), yb))\n",
    "    \n",
    "    lr = 0.5\n",
    "    epochs = 2 # how many laps through the dataset\n",
    "\n",
    "    print(f\"training our model {epochs} epochs with learning rate of {lr}\")\n",
    "    for epoch in range(epochs):\n",
    "        # this 'i' is sometimes called a \"period\" when iterating through the dataset\n",
    "        for i in range((n - 1) // bs + 1): # why n-1?\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs # now we have a range [i*bs, (i+1)*bs)\n",
    "            \n",
    "            x_mini_batch = x_train[start_i: end_i]\n",
    "            y_mini_batch = y_train[start_i: end_i]\n",
    "            \n",
    "            prediction = model(x_mini_batch) # prediction matrix of matrices (\"tensor\")\n",
    "            loss = loss_func(prediction, y_mini_batch)\n",
    "            \n",
    "            if i % 100 == 0: # do some logging every 20th batch\n",
    "                print(f' Loss of {loss}, with batch accuracy of {accuracy(prediction, y_mini_batch)}')\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                weights -= weights.grad * lr\n",
    "                bias -= bias.grad * lr\n",
    "                weights.grad.zero_()\n",
    "                bias.grad.zero_()\n",
    "    \n",
    "    print('Final Loss', loss_func(model(xb), yb), 'and Accuracy', accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction #1 loss: tensor(2.3454, grad_fn=<NllLossBackward>) Accuracy: tensor(0.0938)\n",
      "training our model 2 epochs with learning rate of 0.5\n",
      " Loss of 2.3454432487487793, with batch accuracy of 0.09375\n",
      " Loss of 0.317440390586853, with batch accuracy of 0.890625\n",
      " Loss of 0.3076491355895996, with batch accuracy of 0.859375\n",
      " Loss of 0.39076757431030273, with batch accuracy of 0.90625\n",
      " Loss of 0.23709739744663239, with batch accuracy of 0.890625\n",
      " Loss of 0.38853177428245544, with batch accuracy of 0.890625\n",
      " Loss of 0.2624582052230835, with batch accuracy of 0.890625\n",
      " Loss of 0.3857908248901367, with batch accuracy of 0.90625\n",
      " Loss of 0.28397250175476074, with batch accuracy of 0.921875\n",
      " Loss of 0.26196223497390747, with batch accuracy of 0.921875\n",
      " Loss of 0.1962507665157318, with batch accuracy of 0.90625\n",
      " Loss of 0.3435472249984741, with batch accuracy of 0.921875\n",
      " Loss of 0.21012549102306366, with batch accuracy of 0.921875\n",
      " Loss of 0.35814568400382996, with batch accuracy of 0.890625\n",
      " Loss of 0.22585365176200867, with batch accuracy of 0.921875\n",
      " Loss of 0.36770039796829224, with batch accuracy of 0.875\n",
      "Final Loss tensor(0.2294, grad_fn=<NllLossBackward>) and Accuracy tensor(0.9531)\n"
     ]
    }
   ],
   "source": [
    "functional_linear_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor into PyTorch nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    '''Encapsulates the weights, bias, and forward() method'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "lr = 0.5\n",
    "xb = x_train[0:bs] # one batch\n",
    "yb = y_train[0:bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3429, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(F.cross_entropy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1544, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9375)\n",
      "Loss: tensor(0.2990, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2586, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9375)\n",
      "Loss: tensor(0.2205, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.1578, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.4692, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2976, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9375)\n",
      "Loss: tensor(0.2169, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.1869, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9062)\n",
      "Loss: tensor(0.1216, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3173, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.3043, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.1762, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9688)\n",
      "Loss: tensor(0.1503, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3253, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.1515, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.1530, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9688)\n",
      "Loss: tensor(0.2985, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2576, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9375)\n",
      "Loss: tensor(0.2201, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.1571, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.4701, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2968, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9375)\n",
      "Loss: tensor(0.2158, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.1868, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9062)\n",
      "Loss: tensor(0.1204, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3158, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.3048, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.1751, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9688)\n",
      "Loss: tensor(0.1492, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3239, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.1514, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def fit(epochs=2):\n",
    "    '''Train the model to fit to the data distribution'''\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        steps = (n - 1) // bs + 1\n",
    "        for i in range(steps):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb, yb = x_train[start_i:end_i], y_train[start_i:end_i]\n",
    "            \n",
    "            pred = model(xb)\n",
    "            loss = F.cross_entropy(pred, yb)\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(\"Loss:\", loss)\n",
    "            if i % 100 == 0:\n",
    "                print(\"Accuracy:\", accuracy(pred, yb))\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor for nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3260, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_global_loss():\n",
    "    print('Global Loss:', loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(2.3260, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.0625)\n",
      "Loss: tensor(0.4135, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3205, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9062)\n",
      "Loss: tensor(0.2697, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2987, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8750)\n",
      "Loss: tensor(0.5576, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3888, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.3000, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2411, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.2113, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3863, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.3603, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2629, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.2308, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3723, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9062)\n",
      "Loss: tensor(0.2131, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2797, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.3349, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2614, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.1955, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.1964, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9062)\n",
      "Loss: tensor(0.4878, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3433, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9219)\n",
      "Loss: tensor(0.2536, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2128, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9375)\n",
      "Loss: tensor(0.1748, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3579, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.3226, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.2261, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.9062)\n",
      "Loss: tensor(0.2003, grad_fn=<NllLossBackward>)\n",
      "Loss: tensor(0.3569, grad_fn=<NllLossBackward>)\n",
      "Accuracy: tensor(0.8906)\n",
      "Loss: tensor(0.1883, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2264, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "fit()\n",
    "\n",
    "print_global_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor to use torch.optim\n",
    "\n",
    "For all your optimization needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Loss: tensor(2.2984, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "print_global_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Loss: tensor(0.0821, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print_global_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor to use Dataset\n",
    "\n",
    "By defining how we index into our dataset, we can iterate and slice like a serial killer.\n",
    "* I haven't yet done the Dataset tutorial, because it might not be as necessary for my work.\n",
    "  + What I have done is read the mentality of the Dataset API when I was working with Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Loss: tensor(2.3320, grad_fn=<NllLossBackward>)\n",
      "Global Loss: tensor(0.0833, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "print_global_loss()\n",
    "\n",
    "for e in range(epochs):\n",
    "    steps = (n - 1) // bs + 1\n",
    "    for i in range(steps):\n",
    "        start = i * bs\n",
    "        xb, yb = train_ds[start:start + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print_global_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tower of abstractions: DataLoader\n",
    "\n",
    "You can wrap a `Dataset` in a `DataLoader` to get a better grip on things.\n",
    "* Even easier to iterate. We don't control the indices anymore!\n",
    "\n",
    "In so doing, we:\n",
    "* Off-load batching\n",
    "* Remove concerns for \"Index Out of Bounds\" errors\n",
    "* Leverage the ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Loss: tensor(2.2560, grad_fn=<NllLossBackward>)\n",
      "Global Loss: tensor(0.0813, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "print_global_loss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print_global_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick aside\n",
    "\n",
    "Till this point, we've been doing a lot of trimming and refactoring to get a solid training loop\n",
    "* We're down to ~7 lines of code (which is a something of an ideal)\n",
    "* We __haven't__ given any consideration to a "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
